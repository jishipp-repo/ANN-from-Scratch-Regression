{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import math\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "print('TensorFlow Version:', tf.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = r\"Data.xlsx\"\n",
    "\n",
    "FPX_Data = pd.read_excel(excel_path, sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPX_Train = FPX_Data['Train'] \n",
    "colnames = np.array(FPX_Train.columns)\n",
    "FPX_Train = np.array(FPX_Train)\n",
    "FPX_Train_X = FPX_Train[:,2:]\n",
    "FPX_Train_y = FPX_Train[:,1]\n",
    "FPX_Train_crudes = FPX_Train[:,0]\n",
    "\n",
    "FPX_Test = FPX_Data['Test']\n",
    "FPX_Test = np.array(FPX_Test)\n",
    "FPX_Test_X = FPX_Test[:,2:]\n",
    "FPX_Test_y = FPX_Test[:,1]\n",
    "FPX_Test_crudes = FPX_Test[:,0]\n",
    "\n",
    "FPX_Val = FPX_Data['Validation']\n",
    "FPX_Val = np.array(FPX_Val)\n",
    "FPX_Val_X = FPX_Val[:,2:]\n",
    "FPX_Val_y = FPX_Val[:,1]\n",
    "FPX_Val_crudes = FPX_Val[:,0]\n",
    "\n",
    "FPX_Rem = FPX_Data['Validation 2'] \n",
    "FPX_Rem = np.array(FPX_Rem)\n",
    "FPX_Rem_X = FPX_Rem[:,2:]\n",
    "FPX_Rem_y = FPX_Rem[:,1]\n",
    "FPX_Rem_crudes = FPX_Rem[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_r2(x_list, y_list):\n",
    "    n = len(x_list)\n",
    "    x_bar = sum(x_list)/n\n",
    "    y_bar = sum(y_list)/n\n",
    "    x_std = math.sqrt(sum([(xi-x_bar)**2 for xi in x_list])/(n-1))\n",
    "    y_std = math.sqrt(sum([(yi-y_bar)**2 for yi in y_list])/(n-1))\n",
    "    zx = [(xi-x_bar)/x_std for xi in x_list]\n",
    "    zy = [(yi-y_bar)/y_std for yi in y_list]\n",
    "    r = sum(zxi*zyi for zxi, zyi in zip(zx, zy))/(n-1)\n",
    "    return r**2\n",
    "\n",
    "def plot_results(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    epochs = len(history['val_loss'])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs), history['val_loss'], label='Val Loss')\n",
    "    plt.plot(range(epochs), history['train_loss'], label='Train Loss')\n",
    "    plt.xticks(list(range(epochs)))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(epochs), history['val_acc'], label='Val Acc')\n",
    "    plt.xticks(list(range(epochs)))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred, name='squared_error', threshold=1):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < threshold\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    absolute_loss = tf.abs(error)\n",
    "    linear_loss = threshold * absolute_loss - threshold**2 / 2\n",
    "    \n",
    "    if name == 'squared_error':\n",
    "        return squared_loss\n",
    "    elif name == 'absolute_error':\n",
    "        return absolute_loss\n",
    "    elif name == 'huber_loss':\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def weight_initializer(shape, name='default_normal', minval_unif=0, maxval_unif=None, dtype=tf.float32):\n",
    "    if name == 'default_normal':\n",
    "        return tf.random.normal(shape, dtype=dtype)\n",
    "    elif name == 'default_uniform':\n",
    "        return tf.random.uniform(shape, minval=minval_unif, maxval=maxval_unif, dtype=dtype)\n",
    "    elif name == 'glorot_normal':\n",
    "        stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "        return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "    elif name == 'glorot_uniform':\n",
    "        r = tf.sqrt(6. / (shape[0] + shape[1]))\n",
    "        return tf.random.uniform(shape, minval=-r, maxval=r, dtype=dtype)\n",
    "    elif name == 'he_normal':\n",
    "        stddev = tf.sqrt(2. / (shape[0]))\n",
    "        return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "    elif name == 'he_uniform':\n",
    "        r = tf.sqrt(6. / (shape[0]))\n",
    "        return tf.random.uniform(shape, minval=-r, maxval=r, dtype=dtype)\n",
    "    elif name == 'lecun_normal':\n",
    "        stddev = tf.sqrt(1. / (shape[0]))\n",
    "        return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "    elif name == 'lecun_uniform':\n",
    "        r = tf.sqrt(3. / (shape[0]))\n",
    "        return tf.random.uniform(shape, minval=-r, maxval=r, dtype=dtype)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "def relu(weights): \n",
    "    return np.where(weights < 0., np.zeros_like(weights), weights)\n",
    "\n",
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "def selu(z, scale=1.0507, alpha=1.6732):\n",
    "    return scale * elu(z, alpha)\n",
    "\n",
    "def l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # Initialization of network parameters\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.L = len(layers)\n",
    "        self.num_features = layers[0]\n",
    "        \n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        \n",
    "        self.dW = {}\n",
    "        self.db = {}\n",
    "        \n",
    "        self.setup()\n",
    "       \n",
    "    # Initialize the weights \n",
    "    def setup(self):\n",
    "        for i in range(1, self.L):\n",
    "            self.W[i] = tf.Variable(weight_initializer(shape=(self.layers[i], self.layers[i-1])), name='lecun_uniform')\n",
    "            self.b[i] = tf.Variable(weight_initializer(shape=(self.layers[i], 1), name='lecun_uniform'))\n",
    "    \n",
    "    # Forward Pass to iterate over each layer and get the final layer activation output\n",
    "    def forward_pass(self, X):\n",
    "        A = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        for i in range(1, self.L):\n",
    "            Z = tf.matmul(A, tf.transpose(self.W[i])) + tf.transpose(self.b[i])\n",
    "            if i != self.L-1:\n",
    "                A = tf.nn.sigmoid(Z)\n",
    "            else:\n",
    "                A = Z\n",
    "        return A\n",
    "    \n",
    "    # Loss function: \n",
    "    def compute_loss(self, A, Y):\n",
    "        loss = loss_function(y_true=Y, y_pred=A, name='squared_error')\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    # Weight & Bias update using learning rate and their respective gradients.\n",
    "    def update_params(self, lr):\n",
    "        for i in range(1, self.L):\n",
    "            self.W[i].assign_sub(lr * self.dW[i])\n",
    "            self.b[i].assign_sub(lr * self.db[i])\n",
    "    \n",
    "    # Predict function: Forward pass --> ReLu\n",
    "    def predict(self, X):\n",
    "        A = self.forward_pass(X)\n",
    "        return tf.nn.relu(A) \n",
    "    \n",
    "    # Helper function to get the network info\n",
    "    def info(self):\n",
    "        num_params = 0\n",
    "        for i in range(1, self.L):\n",
    "            num_params += self.W[i].shape[0] * self.W[i].shape[1]\n",
    "            num_params += self.b[i].shape[0]\n",
    "        print('Input Features:', self.num_features)\n",
    "        print('Hidden Layers:')\n",
    "        print('--------------')\n",
    "        for i in range(1, self.L-1):\n",
    "            print('Layer {}, Units {}'.format(i, self.layers[i]))\n",
    "        print('--------------')\n",
    "        print('Number of parameters:', num_params)\n",
    "    \n",
    "    # Train the network on batch of samples\n",
    "    def train_on_batch(self, X, Y, lr):\n",
    "        X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            A = self.forward_pass(X)\n",
    "            loss = self.compute_loss(A, Y)\n",
    "        for i in range(1, self.L):\n",
    "            self.dW[i] = tape.gradient(loss, self.W[i])\n",
    "            self.db[i] = tape.gradient(loss, self.b[i])\n",
    "        \n",
    "        del tape\n",
    "        self.update_params(lr)\n",
    "        return loss.numpy()\n",
    "    \n",
    "    # Train the network on train and validate on test  samples\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs, steps_per_epoch, batch_size, lr):\n",
    "        history = {\n",
    "            'val_loss': [],\n",
    "            'train_loss': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        for e in range(0, epochs):\n",
    "            epoch_train_loss = 0.\n",
    "            print('Epoch {}'.format(e), end='.')\n",
    "            for i in range(0, steps_per_epoch):\n",
    "                x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
    "                y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                \n",
    "                batch_loss = self.train_on_batch(x_batch, y_batch, lr)\n",
    "                epoch_train_loss += batch_loss\n",
    "                \n",
    "            history['train_loss'].append(epoch_train_loss/steps_per_epoch)\n",
    "            val_A = self.forward_pass(x_test)\n",
    "            val_loss = self.compute_loss(val_A, y_test).numpy()\n",
    "            history['val_loss'].append(val_loss)\n",
    "            val_preds = self.predict(x_test)\n",
    "            #val_acc = np.mean(np.argmax(y_test, axis =1) == val_preds.numpy())\n",
    "            val_acc = tf.compat.v1.losses.mean_squared_error(labels=y_test, predictions=val_preds.numpy()).numpy()\n",
    "            history['val_acc'].append(val_acc)            \n",
    "            train_preds = self.predict(x_train)\n",
    "            train_acc = tf.compat.v1.losses.mean_squared_error(labels=y_train, predictions=train_preds.numpy()).numpy()\n",
    "            print('Train acc: ', train_acc, '\\tVal acc: ', val_acc)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coln = 16\n",
    "\n",
    "Train_X_inp = FPX_Train_X[:,:coln]\n",
    "Train_X_spec = FPX_Train_X[:,coln:]\n",
    "y_train = FPX_Train_y[:]\n",
    "crude_train = FPX_Train_crudes[:]\n",
    "Test_X_inp = FPX_Test_X[:,:coln]\n",
    "Test_X_spec = FPX_Test_X[:,coln:]\n",
    "y_test = FPX_Test_y[:]\n",
    "crude_test = FPX_Test_crudes[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X_all = np.concatenate((np.array(Train_X_inp), np.array(Train_X_spec)), axis=1)\n",
    "Test_X_all = np.concatenate((np.array(Test_X_inp), np.array(Test_X_spec)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = (Train_X_spec, y_train.reshape(-1,1)), (Test_X_spec, y_test.reshape(-1,1))\n",
    "#(x_train, y_train), (x_test, y_test) = (Train_X_inp, y_train.reshape(-1,1)), (Test_X_inp, y_test.reshape(-1,1))\n",
    "(x_train, y_train), (x_test, y_test) = (Train_X_all, y_train.reshape(-1,1)), (Test_X_all, y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.constant(x_train, dtype=tf.float32)\n",
    "y_train = tf.constant(y_train, dtype=tf.float32)\n",
    "x_test = tf.constant(x_test, dtype=tf.float32)\n",
    "y_test = tf.constant(y_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork([int(x_train.shape[1]), 100, 50, 20, 5, 1])\n",
    "net.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =127\n",
    "epochs = 100\n",
    "steps_per_epoch = int(x_train.shape[0]//batch_size)\n",
    "lr = 3e-3\n",
    "print('Steps per epoch', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = net.train(\n",
    "    x_train, y_train, x_test, y_test,\n",
    "    epochs, steps_per_epoch, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "y_true_train, y_pred_train = y_train.numpy(), net.predict(x_train).numpy()\n",
    "y_true_test, y_pred_test = y_test.numpy(), net.predict(x_test).numpy()\n",
    "\n",
    "axs.scatter(y_true_train, y_pred_train, label = 'Train', s=20, facecolors='none', edgecolors='b')\n",
    "axs.scatter(y_true_test, y_pred_test, label = 'Test', s=20, facecolors='none', edgecolors='r')\n",
    "\n",
    "xlab = str('True ' + '' + '\\n\\n' + \"MSE(train): \" + \n",
    "    str(round(mean_squared_error(y_true_train, y_pred_train), 3))  \n",
    "    + '\\n $R^{2}(train):$' +\n",
    "    str(round(get_r2(y_true_train, y_pred_train)[0], 3)) \n",
    "    + \"\\nMSE(test): \" + \n",
    "    str(round(mean_squared_error(y_true_test, y_pred_test), 3))  \n",
    "    + '\\n $R^{2}(test):$' +\n",
    "    str(round(get_r2(y_true_test, y_pred_test)[0], 3)))\n",
    "\n",
    "xx1 = np.array([min(min(y_true_train),min(y_pred_train)) - 5,max(max(y_true_train),max(y_pred_train)) + 5])\n",
    "xx2 = np.array([min(min(y_true_test),min(y_pred_test)) - 5,max(max(y_true_test),max(y_pred_test)) + 5])\n",
    "xx  = np.array([min(xx1[0],xx2[0]), max(xx1[1],xx2[1])])\n",
    "axs.plot(xx,xx,'--',color='grey')\n",
    "axs.set_aspect('equal', 'box')\n",
    "plt.xlim(xx[0], xx[1])\n",
    "plt.ylim(xx[0], xx[1])\n",
    "_ = axs.set(xlabel= xlab, ylabel='Prediction ' + '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_x = net.W.copy()\n",
    "b_x = net.b.copy()\n",
    "\n",
    "for ky in W_x:\n",
    "    W_x[ky] = W_x[ky].numpy().tolist()\n",
    "\n",
    "for ky in b_x:\n",
    "    b_x[ky] = b_x[ky].numpy().tolist()\n",
    "\n",
    "model_dict = {'weights': W_x, 'bias': b_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json   \n",
    "with open(\"model_dict.json\", \"w\") as outfile:  \n",
    "    json.dump(model_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(weights): \n",
    "    return np.where(weights < 0., np.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Weights and Bias\n",
    "with open('model_dict.json') as f: \n",
    "    model_deploy = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkDeploy:\n",
    "    # Initialization of network parameters\n",
    "    def __init__(self, model_load):\n",
    "        \n",
    "        self.model_load = model_load\n",
    "        \n",
    "        layers = self.model_load['weights'].keys()\n",
    "        self.L = len(layers) + 1\n",
    "        \n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        \n",
    "        self.setup()       \n",
    "    # Load the weights \n",
    "    def setup(self):\n",
    "        for i in range(1, self.L):\n",
    "            self.W[i] = np.array(self.model_load['weights'][str(i)], dtype=np.float32)\n",
    "            self.b[i] = np.array(self.model_load['bias'][str(i)], dtype=np.float32)\n",
    "    \n",
    "    # Forward Pass to iterate over each layer and get the final layer activation output\n",
    "    def forward_pass(self, X):\n",
    "        A = X\n",
    "        for i in range(1, self.L):\n",
    "            Z = np.matmul(A, np.transpose(self.W[i])) + np.transpose(self.b[i])\n",
    "            if i != self.L-1:\n",
    "                A = logit(Z)\n",
    "            else:\n",
    "                A = Z\n",
    "        return A\n",
    "    \n",
    "    # Predict function: Forward pass --> ReLu\n",
    "    def predict(self, X):\n",
    "        A = self.forward_pass(X)\n",
    "        return relu(A) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dep = NeuralNetworkDeploy(model_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dep.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.predict(x_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
